{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3622d34c-dd6f-410d-8196-f5884bd7836a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Snowpark For Python -- Natural Language Processing using spaCy\n",
    "\n",
    "### In this session, we will cover:\n",
    "\n",
    "* Snowpark for Python Installation\n",
    "* Creating Session object and connecting to Snowflake\n",
    "* Reading and loading data from Snowflake table into Snowpark DataFrame\n",
    "* Perfoming Exploratory Data Analysis (EDA) on Snowpark DataFrame\n",
    "* Creating User-Defined Function (UDF)\n",
    "* Using pre-trained scikit-learn model for inference in UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd6fec",
   "metadata": {},
   "source": [
    "### Snowpark For Python Installation\n",
    "- conda create --name snowpark -c https://repo.anaconda.com/pkgs/snowflake python=3.8\n",
    "- conda activate snowpark\n",
    "- pip install \"snowflake-snowpark-python[pandas]\"\n",
    "- pip install ipykernel\n",
    "- pip install cachetools\n",
    "- pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21210110-09fb-41ed-a248-1ce6b1f21b47",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80562f60-bf0a-40cb-bd12-55f8066f595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import udf, count, col, year, call_udf, array_construct\n",
    "from snowflake.snowpark.types import Variant\n",
    "from snowflake.snowpark.version import VERSION\n",
    "# Misc\n",
    "import pandas as pd\n",
    "import json\n",
    "from cachetools import cached\n",
    "\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf9295-1fa3-4d77-8fb5-da71cab73475",
   "metadata": {},
   "source": [
    "### Establish Secure Connection to Snowflake\n",
    "\n",
    "##### *Options: Username/Password, MFA, OAuth, Okta, SSO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da9f106-f5b5-42a6-a341-22f282077484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse                   : DASH_XL\n",
      "Database                    : DASH_DB\n",
      "Schema                      : DASH_SCHEMA\n",
      "Snowflake version           : 6.18.3\n",
      "Snowpark for Python version : 0.7.0\n"
     ]
    }
   ],
   "source": [
    "connection_parameters = json.load(open('../connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "snowflake_environment = session.sql('select current_warehouse(), current_database(), current_schema(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('Warehouse                   : {}'.format(snowflake_environment[0][0]))\n",
    "print('Database                    : {}'.format(snowflake_environment[0][1]))\n",
    "print('Schema                      : {}'.format(snowflake_environment[0][2]))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][3]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a1f8e-7d9f-46fc-940e-85d0731e82b8",
   "metadata": {},
   "source": [
    "### Load Amazon Reviews data from Snowflake table into Snowpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dcb70b-4c34-47df-aef1-d26588d0af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MARKETPLACE\"  |\"REVIEW_ID\"     |\"PRODUCT_ID\"  |\"PRODUCT_PARENT\"  |\"PRODUCT_TITLE\"                                     |\"PRODUCT_CATEGORY\"  |\"STAR_RATING\"  |\"HELPFUL_VOTES\"  |\"TOTAL_VOTES\"  |\"VINE\"  |\"VERIFIED_PURCHASE\"  |\"REVIEW_HEADLINE\"                                   |\"REVIEW_BODY\"                                       |\"REVIEW_DATE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|US             |R2W9DA68EPF0FV  |0679433740    |925934976         |Paradise                                            |Books               |3              |0                |1              |N       |N                    |complex but enjoyable                               |I read this book for a book club assignment.  I...  |1998-07-06     |\n",
      "|US             |R2PQMUNTNC11BT  |0670880728    |738261988         |Bridget Jones's Diary                               |Books               |5              |1                |1              |N       |N                    |When are they bringing out Diary #2?  Keep 'em ...  |I thought that Bridget was so completely and ut...  |1998-07-06     |\n",
      "|US             |R1H3J29NUX6YV0  |0060920084    |754387444         |The Lost Continent: Travels in Small-Town America   |Books               |3              |5                |7              |N       |N                    |Clever but cruel                                    |Bryson's breakneck tour through small American ...  |1998-07-06     |\n",
      "|US             |R16TBQZL8TQTO2  |0553208845    |36215520          |Siddhartha                                          |Books               |5              |0                |0              |N       |N                    |A Beautiful Story                                   |This book is so wonderfully written that it tak...  |1998-07-06     |\n",
      "|US             |RW3PHHVONKQRK   |B000006NPY    |940428624         |Adore                                               |Music               |4              |0                |0              |N       |N                    |You'll adore it!                                    |It's different from the other cd's, but it's a ...  |1998-07-06     |\n",
      "|US             |R1SU6AE9W0XRNU  |B000002N2P    |820995200         |Green Day - Insomniac                               |Music               |1              |3                |22             |N       |N                    |I wish I could give it ZERO stars                   |This album is a discrace to music.  It is a dis...  |1998-07-07     |\n",
      "|US             |R5IT6PFPOFYTO   |B000002NHN    |95315084          |The Book of Secrets                                 |Music               |5              |0                |0              |N       |N                    |Even in michigan, we love it                        |Loreena McKennitt does it again with a compilat...  |1998-07-07     |\n",
      "|US             |RL790PTSRIVWA   |0783880871    |529898130         |Unnatural Exposure                                  |Books               |1              |0                |0              |N       |N                    |A sloppy effort, too many loose ends.. unsatisf...  |Half a star would do actually ... PC tries to i...  |1998-07-07     |\n",
      "|US             |R17DHSAWYNIBD7  |B000002LIM    |375271481         |Batman: Original Motion Picture Score               |Music               |5              |2                |2              |N       |N                    |One of the All-Time Great Movie Themes              |Danny Elfman has composed a wonderfully origina...  |1998-07-07     |\n",
      "|US             |R2FEX1S8FAHSDZ  |0887306667    |381720534         |The 22 Immutable Laws of Marketing:  Violate Th...  |Books               |4              |1                |1              |N       |N                    |The Marketing Cooking book                          |A well written book about classic marketing app...  |1998-07-07     |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df = session.table(\"SUMMIT_AMAZON_REVIEWS_DB.DASH_SCHEMA.AMAZON_REVIEWS\")\n",
    "snow_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd56c3a-75a6-4166-b940-4d65d512d2f2",
   "metadata": {},
   "source": [
    "### Number of records per STAR_RATING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da3775a-a8ea-4398-b93f-676a1fa1422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "|\"STAR_RATING\"  |\"COUNT(STAR_RATING)\"  |\n",
      "----------------------------------------\n",
      "|5              |16697                 |\n",
      "|1              |1709                  |\n",
      "|3              |1612                  |\n",
      "|2              |1155                  |\n",
      "|4              |3827                  |\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df.group_by('STAR_RATING').agg(count('STAR_RATING')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed13bd",
   "metadata": {},
   "source": [
    "### Number of records per PRODUCT_CATEGORY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41315521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "|\"PRODUCT_CATEGORY\"  |\"COUNT(PRODUCT_CATEGORY)\"  |\n",
      "--------------------------------------------------\n",
      "|Video               |1751                       |\n",
      "|Books               |6640                       |\n",
      "|Camera              |5000                       |\n",
      "|Video DVD           |721                        |\n",
      "|Music               |5888                       |\n",
      "|Watches             |5000                       |\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df.group_by('PRODUCT_CATEGORY').agg(count('PRODUCT_CATEGORY')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71314391-e130-44fe-b3dc-f2a8be023e4c",
   "metadata": {},
   "source": [
    "### Number of STAR_RATINGs per YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3b1acf-0ee8-49cf-84b3-9a7b8d7b2f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "|\"YEAR\"  |\"TOTAL_RATINGS\"  |\n",
      "----------------------------\n",
      "|1995    |9                |\n",
      "|1996    |192              |\n",
      "|1997    |1753             |\n",
      "|1998    |11274            |\n",
      "|1999    |1772             |\n",
      "|2015    |10000            |\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df.group_by(year('REVIEW_DATE')).agg(count('STAR_RATING').as_('TOTAL_RATINGS')).with_column_renamed('YEAR(REVIEW_DATE)','YEAR').sort('YEAR').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf78ccc-3132-42a7-b029-c26d89abdb9d",
   "metadata": {},
   "source": [
    "### Missing data ... rows with no STAR_RATING or REVIEW_BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8380095-9ced-4e08-b6f4-fc9686123007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "|\"REVIEW_ID\"     |\"STAR_RATING\"  |\"REVIEW_BODY\"  |\n",
      "--------------------------------------------------\n",
      "|R3R885VN6USBYM  |5              |NULL           |\n",
      "|R3GKZOOU9MQIB8  |5              |NULL           |\n",
      "|R26LKY7Y8QG2Y2  |1              |NULL           |\n",
      "|R3QXY2UIFIUEYI  |4              |NULL           |\n",
      "|R2OJE1F0QFYC8E  |5              |NULL           |\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df = snow_df.filter(col('STAR_RATING').is_null() | col('REVIEW_BODY').is_null()).select(['REVIEW_ID','STAR_RATING','REVIEW_BODY'])\n",
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2fb0f",
   "metadata": {},
   "source": [
    "#### >>>>>>>>>> *Examine Snowpark DataFrame Query* <<<<<<<<<< "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d941dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT \"REVIEW_ID\", \"STAR_RATING\", \"REVIEW_BODY\" FROM ( SELECT  *  FROM ( SELECT  *  FROM (SUMMIT_AMAZON_REVIEWS_DB.DASH_SCHEMA.AMAZON_REVIEWS)) WHERE (\"STAR_RATING\" IS NULL OR \"REVIEW_BODY\" IS NULL))'],\n",
       " 'post_actions': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086b6a2-a017-4717-bf48-f07157536a0b",
   "metadata": {},
   "source": [
    "### Data cleanup -- Remove rows with null values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86b88fd-7471-4e34-8698-3e5540313b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before cleanup    :  25000\n",
      "Records after cleanup     :  24994\n",
      "Number of records removed :  6\n"
     ]
    }
   ],
   "source": [
    "records_before = snow_df.count()\n",
    "print('Records before cleanup    : ',records_before)\n",
    "\n",
    "# Delete rows with missing values\n",
    "snow_df = snow_df.dropna()\n",
    "\n",
    "# Filter out rows with no STAR_RATING or REVIEW_BODY\n",
    "snow_df = snow_df.filter(col('STAR_RATING').is_not_null() | col('REVIEW_BODY').is_not_null())\n",
    "\n",
    "records_after = snow_df.count()\n",
    "print('Records after cleanup     : ',records_after)\n",
    "print('Number of records removed : ',(records_before - records_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b8865-af18-4b1c-b4b3-8f45d73375a3",
   "metadata": {},
   "source": [
    "### User-Defined Function (UDF) for Text Processing Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb54bb-80ba-49f8-a584-3fbbf0afc9ed",
   "metadata": {},
   "source": [
    "* Upload external dependency to an internal stage\n",
    "* Add dependency to the Session for the UDF\n",
    "* Create UDF with additional packages from Snowflake Anaconda Channel\n",
    "* Call UDF on Amaxon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dfc422-fdd9-4432-ad7d-a077694146d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The version of package spacy in the local environment is 3.2.3, which does not fit the criteria for the requirement spacy==2.3.5. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "# Upload dependencies to a stage\n",
    "session.sql(\"create or replace stage dash_udf_imports\").collect()\n",
    "session.file.put(\"file:///Users/ddesai/en_core_web_sm.zip\", \"@dash_udf_imports/\")\n",
    "\n",
    "# Add dependency to the Session for the UDF\n",
    "session.clear_imports()\n",
    "session.add_import('@dash_udf_imports/en_core_web_sm.zip.gz')\n",
    "\n",
    "# Function to download and extract English pipeline in spaCy\n",
    "@cached(cache={})\n",
    "def extract_en_core_web_sm(input_file: str, output_dir: str)-> object:\n",
    "    import zipfile\n",
    "    import spacy\n",
    "            \n",
    "    with zipfile.ZipFile(input_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "        \n",
    "    # load and return the english language small model of spacy\n",
    "    nlp = spacy.load(output_dir + \"/en_core_web_sm/en_core_web_sm-2.3.0\")\n",
    "    return nlp \n",
    "\n",
    "# Create UDF with additional packages from Snowflake Anaconda Channel\n",
    "# -- Remove HTML, tokenize text, lemmatize verbs and remove stop words\n",
    "@udf(name='process_text',session=session,packages=['spacy==2.3.5','beautifulsoup4','cachetools==4.2.2'],replace=True,is_permanent=True,stage_location='dash_udfs')\n",
    "def process_text(text: str) -> str:\n",
    "    import os\n",
    "    import sys\n",
    "    from bs4 import BeautifulSoup \n",
    "    from spacy.tokenizer import Tokenizer\n",
    "                       \n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    \n",
    "    input_file = import_dir + 'en_core_web_sm.zip'\n",
    "    output_dir = '/tmp/en_core_web_sm' + str(os.getpid())\n",
    "    \n",
    "    nlp = extract_en_core_web_sm(input_file,output_dir)\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    \n",
    "    # strip html\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = tokenizer(text)\n",
    "    \n",
    "    # lemmatize verbs and remove stop words\n",
    "    text = [str(t.lemma_) for t in tokens if (t.orth_) not in stop_words] \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714de9b",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> *Examine Query History in Snowsight* <<<<<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58272f5",
   "metadata": {},
   "source": [
    "### Call UDF on Amazon Reviews -- optionally convert results into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed1e8a34-f092-4b0b-96c3-798cbf0af26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 3.46 ms, total: 13.5 ms\n",
      "Wall time: 6.42 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVIEW_BODY</th>\n",
       "      <th>PROCESSED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I read this book for a book club assignment.  I enjoyed but wish I'd had paper and pencil to recordthe particulars of likes and dislikes.  I noticed a similarity to Stephen King of whom I've only read 2 works.  I read it in two days and don't regret the time.  I'll probably read more of Morrison.</td>\n",
       "      <td>['I', 'read', 'book', 'book', 'club', 'assignment.', ' ', 'I', 'enjoy', 'wish', \"I'd\", 'paper', 'pencil', 'recordthe', 'particular', 'like', 'dislikes.', ' ', 'I', 'notice', 'similarity', 'Stephen', 'King', \"I've\", 'read', '2', 'works.', ' ', 'I', 'read', 'day', \"don't\", 'regret', 'time.', ' ', \"I'll\", 'probably', 'read', 'Morrison.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought that Bridget was so completely and utterly lovable!!  It was so easy to get caught up in the sheer ridiculousness of her adventures.  Granted, many of the situations she would get caught up in were indeed a little far fetched, but they were hilarious nonetheless. &lt;br /&gt;However, there were so many crazy &amp;quot;girlie&amp;quot; moments throughout the book (i.e. ruling out Mark Darcy as a &amp;q...</td>\n",
       "      <td>['I', 'think', 'Bridget', 'completely', 'utterly', 'lovable!!', ' ', 'It', 'easy', 'catch', 'sheer', 'ridiculousness', 'adventures.', ' ', 'Granted,', 'situation', 'catch', 'little', 'far', 'fetched,', 'hilarious', 'nonetheless.', 'However,', 'crazy', '\"girlie\"', 'moment', 'book', '(i.e.', 'rule', 'Mark', 'Darcy', '\"potential\"', 'sport', 'cheesey', 'sweater,', 'try', 'pull', 'dinner', 'party',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bryson's breakneck tour through small American towns (his running gag is that he's in search of an elusive perfect nostalgic town he calls &amp;quot;Amalgam&amp;quot;) is terribly witty and occasionally dead-on in its lacerating descriptions of American tackiness and silliness.  But often his insights are laced with the kind of acid you-can't-get-me cruelty that only an expatriate (Bryson lives in Lon...</td>\n",
       "      <td>[\"Bryson's\", 'breakneck', 'tour', 'small', 'American', 'town', '(his', 'run', 'gag', \"he's\", 'search', 'elusive', 'perfect', 'nostalgic', 'town', 'call', '\"Amalgam\")', 'terribly', 'witty', 'occasionally', 'dead-on', 'lacerate', 'description', 'American', 'tackiness', 'silliness.', ' ', 'But', 'insight', 'lace', 'kind', 'acid', \"you-can't-get-me\", 'cruelty', 'expatriate', '(Bryson', 'live', 'Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This book is so wonderfully written that it takes you in and opens your eyes without you trying to comprehend it's message.  It is truly inspirational.  If people found the Alchemist insightful, please try a reading a real book and read Siddhartha.</td>\n",
       "      <td>['This', 'book', 'wonderfully', 'write', 'take', 'open', 'eye', 'try', 'comprehend', \"it's\", 'message.', ' ', 'It', 'truly', 'inspirational.', ' ', 'If', 'people', 'find', 'Alchemist', 'insightful,', 'try', 'read', 'real', 'book', 'read', 'Siddhartha.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's different from the other cd's, but it's a great change. An acustic sound, runs through this cd!</td>\n",
       "      <td>[\"It's\", 'different', \"cd's,\", \"it's\", 'great', 'change.', 'An', 'acustic', 'sound,', 'run', 'cd!']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                       REVIEW_BODY  \\\n",
       "0                                                                                                        I read this book for a book club assignment.  I enjoyed but wish I'd had paper and pencil to recordthe particulars of likes and dislikes.  I noticed a similarity to Stephen King of whom I've only read 2 works.  I read it in two days and don't regret the time.  I'll probably read more of Morrison.   \n",
       "1  I thought that Bridget was so completely and utterly lovable!!  It was so easy to get caught up in the sheer ridiculousness of her adventures.  Granted, many of the situations she would get caught up in were indeed a little far fetched, but they were hilarious nonetheless. <br />However, there were so many crazy &quot;girlie&quot; moments throughout the book (i.e. ruling out Mark Darcy as a &q...   \n",
       "2  Bryson's breakneck tour through small American towns (his running gag is that he's in search of an elusive perfect nostalgic town he calls &quot;Amalgam&quot;) is terribly witty and occasionally dead-on in its lacerating descriptions of American tackiness and silliness.  But often his insights are laced with the kind of acid you-can't-get-me cruelty that only an expatriate (Bryson lives in Lon...   \n",
       "3                                                                                                                                                         This book is so wonderfully written that it takes you in and opens your eyes without you trying to comprehend it's message.  It is truly inspirational.  If people found the Alchemist insightful, please try a reading a real book and read Siddhartha.   \n",
       "4                                                                                                                                                                                                                                                                                                             It's different from the other cd's, but it's a great change. An acustic sound, runs through this cd!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                    PROCESSED_TEXT  \n",
       "0                                                                 ['I', 'read', 'book', 'book', 'club', 'assignment.', ' ', 'I', 'enjoy', 'wish', \"I'd\", 'paper', 'pencil', 'recordthe', 'particular', 'like', 'dislikes.', ' ', 'I', 'notice', 'similarity', 'Stephen', 'King', \"I've\", 'read', '2', 'works.', ' ', 'I', 'read', 'day', \"don't\", 'regret', 'time.', ' ', \"I'll\", 'probably', 'read', 'Morrison.']  \n",
       "1  ['I', 'think', 'Bridget', 'completely', 'utterly', 'lovable!!', ' ', 'It', 'easy', 'catch', 'sheer', 'ridiculousness', 'adventures.', ' ', 'Granted,', 'situation', 'catch', 'little', 'far', 'fetched,', 'hilarious', 'nonetheless.', 'However,', 'crazy', '\"girlie\"', 'moment', 'book', '(i.e.', 'rule', 'Mark', 'Darcy', '\"potential\"', 'sport', 'cheesey', 'sweater,', 'try', 'pull', 'dinner', 'party',...  \n",
       "2  [\"Bryson's\", 'breakneck', 'tour', 'small', 'American', 'town', '(his', 'run', 'gag', \"he's\", 'search', 'elusive', 'perfect', 'nostalgic', 'town', 'call', '\"Amalgam\")', 'terribly', 'witty', 'occasionally', 'dead-on', 'lacerate', 'description', 'American', 'tackiness', 'silliness.', ' ', 'But', 'insight', 'lace', 'kind', 'acid', \"you-can't-get-me\", 'cruelty', 'expatriate', '(Bryson', 'live', 'Lo...  \n",
       "3                                                                                                                                                    ['This', 'book', 'wonderfully', 'write', 'take', 'open', 'eye', 'try', 'comprehend', \"it's\", 'message.', ' ', 'It', 'truly', 'inspirational.', ' ', 'If', 'people', 'find', 'Alchemist', 'insightful,', 'try', 'read', 'real', 'book', 'read', 'Siddhartha.']  \n",
       "4                                                                                                                                                                                                                                                                                                              [\"It's\", 'different', \"cd's,\", \"it's\", 'great', 'change.', 'An', 'acustic', 'sound,', 'run', 'cd!']  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_amazon_reviews = snow_df.limit(10).select('REVIEW_BODY', call_udf(\"process_text\", col(\"REVIEW_BODY\")).as_('PROCESSED_TEXT')).to_pandas()\n",
    "df_amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e59b4-4ce7-4728-aa44-935d6539b2c2",
   "metadata": {},
   "source": [
    "### Model Training in Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ce0bc",
   "metadata": {},
   "source": [
    "#### Load Advertising Data into Snowpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d9ff64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "|\"TV\"   |\"Radio\"  |\"Newspaper\"  |\"Sales\"  |\n",
      "-------------------------------------------\n",
      "|230.1  |37.8     |69.2         |22.1     |\n",
      "|44.5   |39.3     |45.1         |10.4     |\n",
      "|17.2   |45.9     |69.3         |12.0     |\n",
      "|151.5  |41.3     |58.5         |16.5     |\n",
      "|180.8  |10.8     |58.4         |17.9     |\n",
      "|8.7    |48.9     |75.0         |7.2      |\n",
      "|57.5   |32.8     |23.5         |11.8     |\n",
      "|120.2  |19.6     |11.6         |13.2     |\n",
      "|8.6    |2.1      |1.0          |4.8      |\n",
      "|199.8  |2.6      |21.2         |15.6     |\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_budgets = session.table('ADVERTISING_BUDGETS')\n",
    "snow_df_budgets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a040d4b",
   "metadata": {},
   "source": [
    "#### Snowpark Python code to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd815ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sales_prediction_model(session: Session, features_table: str) -> Variant:\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "    import os\n",
    "    from joblib import dump\n",
    "\n",
    "    # Load features\n",
    "    df = session.table(features_table).to_pandas()\n",
    "\n",
    "    # Preprocess the Numeric columns\n",
    "    # We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns. NOTE: High degrees can cause overfitting.\n",
    "    numeric_features = ['TV','Radio','Newspaper']\n",
    "    numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree = 2)),('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine the preprocessed step together using the Column Transformer module\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "    # The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "    parameteres = {}\n",
    "\n",
    "    X = df.drop('Sales', axis = 1)\n",
    "    y = df['Sales']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "    model = GridSearchCV(pipeline, param_grid=parameteres, cv=10)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Upload trained model to a stage\n",
    "    model_output_dir = '/tmp'\n",
    "    model_file = os.path.join(model_output_dir, 'sales_model.joblib')\n",
    "    dump(model, model_file)\n",
    "    session.file.put(model_file, \"@dash_models\",overwrite=True)\n",
    "\n",
    "    # Return model R2 score on train and test data\n",
    "    return {\"R2 score on Train\": model.score(X_train, y_train),\"R2 score on Test\": model.score(X_test, y_test)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9048d",
   "metadata": {},
   "source": [
    "#### Test Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77d54f8b-8021-4065-a316-32b344b3386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'R2 score on Train': 0.9288133512730626, 'R2 score on Test': 0.9533174341074796}\n"
     ]
    }
   ],
   "source": [
    "print(train_sales_prediction_model(session,\"ADVERTISING_BUDGETS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22263505",
   "metadata": {},
   "source": [
    "### Create Stored Procedure to deploy training code on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be255759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.stored_procedure.StoredProcedure at 0x7f7b4897f0d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sproc.register(func=train_sales_prediction_model,name=\"train_sales_prediction_model\",packages=['snowflake-snowpark-python','scikit-learn','joblib'],is_permanent=True,stage_location=\"@dash_sprocs\",replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba80e514",
   "metadata": {},
   "source": [
    "### Execute Stored Procedure to train model and deploy it on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca9b909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"R2 score on Test\": 0.9533174341074796,\n",
      "  \"R2 score on Train\": 0.9288133512730626\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(session.call('train_sales_prediction_model','ADVERTISING_BUDGETS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3bc57",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> *Examine Query History in Snowsight* <<<<<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d4fc0",
   "metadata": {},
   "source": [
    "### Create User-Defined Function for Inference on Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae5d98c-ab01-4b9d-bb58-00be7a22041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "# Add trained model as dependency\n",
    "session.add_import('@dash_models/sales_model.joblib.gz')\n",
    "\n",
    "@udf(name='predict_sales',session=session,packages=['pandas','joblib','scikit-learn'],replace=True,is_permanent=True,stage_location='@dash_udfs')\n",
    "def predict_sales(budget_allocations: list) -> float:\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    from joblib import load\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "    \n",
    "    model_file = import_dir + 'sales_model.joblib.gz'\n",
    "\n",
    "    model = load(model_file)\n",
    "            \n",
    "    features = ['TV','Radio','Newspaper']\n",
    "    df = pd.DataFrame([budget_allocations], columns=features)\n",
    "    sales = round(model.predict(df)[0],2)\n",
    "\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a60f1b28-d4c9-435e-8455-271609a507c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "|\"TV\"   |\"RADIO\"  |\"NEWSPAPER\"  |\"PREDICTED_SALES\"  |\n",
      "-----------------------------------------------------\n",
      "|199.8  |2.6      |21.2         |16.2               |\n",
      "|120.2  |19.6     |11.6         |13.69              |\n",
      "|180.8  |10.8     |58.4         |16.13              |\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = session.create_dataframe([[180.8,10.8,58.4],[120.2,19.6,11.6],[199.8,2.6,21.2]], schema=['TV','Radio','Newspaper'])\n",
    "test_df.select('TV','Radio','Newspaper', \n",
    "    call_udf(\"predict_sales\", array_construct(col(\"TV\"), col(\"Radio\"), col(\"Newspaper\"))).as_(\"PREDICTED_SALES\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5dfd4",
   "metadata": {},
   "source": [
    "### >>>>>>>>>> *Examine Query History in Snowsight* <<<<<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62291ee0",
   "metadata": {},
   "source": [
    "# Code on GitHub\n",
    "\n",
    "### Python Notebook is available at https://github.com/iamontheinet/dash-at-summit-2020"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fb0a37530c0004d75c43dbcefc0b8b6ea2fdc6f87c96f7fd6f8cf43b3f551c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('snowpark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
